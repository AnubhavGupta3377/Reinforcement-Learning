{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYflnFMmiQEC"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fwz95L2WjyRw"
   },
   "outputs": [],
   "source": [
    "### Markov Decision Process for Small Gridworld\n",
    "# 1) State-space -> self.states\n",
    "# 2) Action-space -> self.actions\n",
    "# 3) State transition probabilities -> self.P\n",
    "# 4) Reward function -> self.rewards\n",
    "\n",
    "SHAPE = (4,4)\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
    "\n",
    "# Helper function to convert state index from 2d to 1d\n",
    "def get_state_idx_1d(x,y,shape):\n",
    "    return x*shape[1]+y\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        num_states = np.prod(shape)\n",
    "        num_actions = 4\n",
    "        self.states = np.arange(num_states)\n",
    "        self.actions = [UP,DOWN,LEFT,RIGHT]\n",
    "        self.gamma = 1\n",
    "        rewards = [-1 for j in range(num_states)]\n",
    "        rewards[0] = rewards[num_states-1] = 0\n",
    "        self.rewards = rewards\n",
    "        \n",
    "        P = np.zeros((num_states,num_actions,num_states,2)) # for each state-action pair ((x,y), a) => stores P((x',y')) and expected reward\n",
    "        states = np.arange(num_states).reshape(shape)\n",
    "        iterator = np.nditer(states, flags=['multi_index'])\n",
    "\n",
    "        # Probability of next state\n",
    "        def get_next_state(x,y,a):\n",
    "            if (x==0 and y==0) or (x==shape[0]-1 and y==shape[1]-1):\n",
    "                return get_state_idx_1d(x,y,shape)\n",
    "            nx = x\n",
    "            ny = y\n",
    "            if a == UP:\n",
    "                nx = x-1\n",
    "            elif a == DOWN:\n",
    "                nx = x+1\n",
    "            elif a == LEFT:\n",
    "                ny = y-1\n",
    "            else:\n",
    "                 ny = y+1\n",
    "            if nx < 0 or nx > shape[0]-1:\n",
    "                nx = x\n",
    "            if ny < 0 or ny > shape[1]-1:\n",
    "                ny = y\n",
    "            return get_state_idx_1d(nx, ny, shape)\n",
    "\n",
    "        while not iterator.finished:\n",
    "            x,y = iterator.multi_index\n",
    "            cur_state = get_state_idx_1d(x,y,shape)\n",
    "            for a in {UP,DOWN,LEFT,RIGHT}:\n",
    "                next_state = get_next_state(x,y,a)\n",
    "                P[cur_state][a][next_state][0] = 1\n",
    "                P[cur_state][a][next_state][1] = rewards[cur_state]\n",
    "            iterator.iternext()\n",
    "\n",
    "        self.P = P\n",
    "\n",
    "gridworld = Gridworld(SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhX45EXQj04r"
   },
   "outputs": [],
   "source": [
    "# Value Iteration Algorithm\n",
    "\n",
    "def get_value_grid(values):\n",
    "    n = len(values[0])\n",
    "    dim = int(n**0.5)\n",
    "    return np.round(values,2).reshape((dim,dim))\n",
    "\n",
    "def value_iteration(gridworld, threshold=1e-5):\n",
    "    states = gridworld.states\n",
    "    actions = gridworld.actions\n",
    "    gamma = gridworld.gamma\n",
    "    n = len(states)\n",
    "    m = len(actions)\n",
    "\n",
    "    values = np.random.random((1,n))\n",
    "    values[0][0] = values[0][n-1] = 0\n",
    "\n",
    "    iteration_count = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        policy = np.zeros((n, m))\n",
    "        for s in states:\n",
    "            best_actions = [0]\n",
    "            max_value = -float('inf')\n",
    "            for a in actions:\n",
    "                transition_prob = gridworld.P[s,a,:,0]\n",
    "                reward_fun = gridworld.P[s,a,:,1]\n",
    "                action_value = np.multiply(transition_prob,(reward_fun + gamma*values)).sum()\n",
    "                if action_value > max_value:\n",
    "                    best_actions = [a]\n",
    "                    max_value = action_value\n",
    "            delta = max(delta, abs(max_value-values[0][s]))\n",
    "            values[0][s] = max_value\n",
    "            policy[s][best_actions[0]] = 1\n",
    "        iteration_count += 1\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    \n",
    "    print(\"Found optimal policy in {} iterations\\n\".format(iteration_count))\n",
    "    print(\"Optimal policy (action probability distribution):\\n{}\\n\".format(policy))\n",
    "    print(\"Optimal state-value function:\\n{}\\n\".format(values))\n",
    "    print(\"Optimal state-value function (in grid):\\n{}\\n\".format(get_value_grid(values)))\n",
    "    return (values,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "t0jXdMvyp1wC",
    "outputId": "c1881bb6-a11f-461c-a47f-6ae3f3a0b77d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found optimal policy in 5 iterations\n",
      "\n",
      "Optimal policy (action probability distribution):\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Optimal state-value function:\n",
      "[[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]]\n",
      "\n",
      "Optimal state-value function (in grid):\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values, policy = value_iteration(gridworld, threshold=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tn313eX_p8YX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Gridworld Value_Iteration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
