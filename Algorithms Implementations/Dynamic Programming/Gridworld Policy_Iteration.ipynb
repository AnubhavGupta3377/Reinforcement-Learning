{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kJU94BM2W7X"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbWhmgcU5jA0"
   },
   "outputs": [],
   "source": [
    "### Markov Decision Process for Small Gridworld\n",
    "# 1) State-space -> self.states\n",
    "# 2) Action-space -> self.actions\n",
    "# 3) State transition probabilities -> self.P\n",
    "# 4) Reward function -> self.rewards\n",
    "\n",
    "SHAPE = (4,4)\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3\n",
    "\n",
    "# Helper function to convert state index from 2d to 1d\n",
    "def get_state_idx_1d(x,y,shape):\n",
    "    return x*shape[1]+y\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        num_states = np.prod(shape)\n",
    "        num_actions = 4\n",
    "        self.states = np.arange(num_states)\n",
    "        self.actions = [UP,DOWN,LEFT,RIGHT]\n",
    "        self.gamma = 1\n",
    "        rewards = [-1 for j in range(num_states)]\n",
    "        rewards[0] = rewards[num_states-1] = 0\n",
    "        self.rewards = rewards\n",
    "        \n",
    "        P = np.zeros((num_states,num_actions,num_states,2)) # for each state-action pair ((x,y), a) => stores P((x',y')) and expected reward\n",
    "        states = np.arange(num_states).reshape(shape)\n",
    "        iterator = np.nditer(states, flags=['multi_index'])\n",
    "\n",
    "        # Probability of next state\n",
    "        def get_next_state(x,y,a):\n",
    "            if (x==0 and y==0) or (x==shape[0]-1 and y==shape[1]-1):\n",
    "                return get_state_idx_1d(x,y,shape)\n",
    "            nx = x\n",
    "            ny = y\n",
    "            if a == UP:\n",
    "                nx = x-1\n",
    "            elif a == DOWN:\n",
    "                nx = x+1\n",
    "            elif a == LEFT:\n",
    "                ny = y-1\n",
    "            else:\n",
    "                 ny = y+1\n",
    "            if nx < 0 or nx > shape[0]-1:\n",
    "                nx = x\n",
    "            if ny < 0 or ny > shape[1]-1:\n",
    "                ny = y\n",
    "            return get_state_idx_1d(nx, ny, shape)\n",
    "\n",
    "        while not iterator.finished:\n",
    "            x,y = iterator.multi_index\n",
    "            cur_state = get_state_idx_1d(x,y,shape)\n",
    "            for a in {UP,DOWN,LEFT,RIGHT}:\n",
    "                next_state = get_next_state(x,y,a)\n",
    "                P[cur_state][a][next_state][0] = 1\n",
    "                P[cur_state][a][next_state][1] = rewards[cur_state]\n",
    "            iterator.iternext()\n",
    "\n",
    "        self.P = P\n",
    "\n",
    "gridworld = Gridworld(SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tm-XJyBK5k6M"
   },
   "outputs": [],
   "source": [
    "# Policy Evaluation Algorithm\n",
    "\n",
    "def get_value_grid(values):\n",
    "    n = len(values[0])\n",
    "    dim = int(n**0.5)\n",
    "    return np.round(values,2).reshape((dim,dim))\n",
    "\n",
    "def policy_evaluation(policy, gridworld, threshold=1e-5):\n",
    "    states = gridworld.states\n",
    "    actions = gridworld.actions\n",
    "    gamma = gridworld.gamma\n",
    "    n = len(states)\n",
    "    \n",
    "    values = np.random.random((1,n))\n",
    "    values[0][0] = values[0][n-1] = 0\n",
    "        \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            prev_val = values[0][s]\n",
    "            temp = 0\n",
    "            for a in actions:\n",
    "                transition_prob = gridworld.P[s,a,:,0]\n",
    "                reward_fun = gridworld.P[s,a,:,1]\n",
    "                temp += policy[s][a] * np.multiply(transition_prob,(reward_fun + gamma*values)).sum()\n",
    "            values[0][s] = temp\n",
    "            delta = max(delta, abs(values[0][s] - prev_val))\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23p10Exm6OBp"
   },
   "outputs": [],
   "source": [
    "# Policy Improvement Algorithm\n",
    "\n",
    "def policy_improvement(policy, values, gridworld):\n",
    "    states = gridworld.states\n",
    "    actions = gridworld.actions\n",
    "    gamma = gridworld.gamma\n",
    "    n = len(states)\n",
    "    m = len(actions)\n",
    "    \n",
    "    new_policy = np.zeros((n, m))\n",
    "    has_improved = False\n",
    "    for s in states:\n",
    "        best_actions = [0]\n",
    "        max_value = -float('inf')\n",
    "        for a in actions:\n",
    "            transition_prob = gridworld.P[s,a,:,0]\n",
    "            reward_fun = gridworld.P[s,a,:,1]\n",
    "            action_value = np.multiply(transition_prob,(reward_fun + gamma*values)).sum()\n",
    "            if action_value > max_value:\n",
    "                best_actions = [a]\n",
    "            elif action_value == max_value:\n",
    "                best_actions.append(a)\n",
    "            max_value = max(max_value, action_value)\n",
    "\n",
    "        prev_action = policy[s].argmax()\n",
    "        if prev_action not in best_actions:\n",
    "            has_improved = True\n",
    "        new_policy[s][best_actions[0]] = 1\n",
    "        \n",
    "    if has_improved:\n",
    "        return (new_policy, True)\n",
    "    return (policy, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "BwIL4ura5m7l",
    "outputId": "a0a658ea-81f6-448f-b906-b9a90d2cf2eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function of original policy:\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      "Value function of new policy after policy improvement:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states = gridworld.states\n",
    "actions = gridworld.actions\n",
    "policy = np.zeros((len(states),len(actions)))+0.25\n",
    "\n",
    "values = policy_evaluation(policy, gridworld, threshold=1e-5)\n",
    "new_policy, is_improvement = policy_improvement(policy, values, gridworld)\n",
    "print(\"Value function of original policy:\\n{}\\n\".format(get_value_grid(values)))\n",
    "new_values = policy_evaluation(new_policy, gridworld, threshold=1e-5)\n",
    "print(\"Value function of new policy after policy improvement:\\n{}\\n\".format(get_value_grid(new_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkk2G-0p_vbX"
   },
   "outputs": [],
   "source": [
    "# Policy Iteration Algorithm\n",
    "\n",
    "def policy_iteration(gridworld, threshold = 1e-5):\n",
    "    states = gridworld.states\n",
    "    actions = gridworld.actions\n",
    "\n",
    "    policy = np.zeros((len(states),len(actions)))+0.25\n",
    "    iteration_count = 0\n",
    "\n",
    "    while True:\n",
    "        values = policy_evaluation(policy, gridworld, threshold=threshold)\n",
    "        policy, is_improvement = policy_improvement(policy, values, gridworld)\n",
    "        iteration_count += 1\n",
    "        if not is_improvement:\n",
    "            break\n",
    "\n",
    "    print(\"Found optimal policy in {} iterations\\n\".format(iteration_count))\n",
    "    print(\"Optimal policy (action probability distribution):\\n{}\\n\".format(policy))\n",
    "    print(\"Optimal state-value function:\\n{}\\n\".format(values))\n",
    "    print(\"Optimal state-value function (in grid):\\n{}\\n\".format(get_value_grid(values)))\n",
    "\n",
    "    return (policy,values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "7i3MayQ8AJac",
    "outputId": "40caced8-56ad-4d85-8245-8ad5a84ee7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found optimal policy in 2 iterations\n",
      "\n",
      "Optimal policy (action probability distribution):\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Optimal state-value function:\n",
      "[[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]]\n",
      "\n",
      "Optimal state-value function (in grid):\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, value = policy_iteration(gridworld, threshold=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-Ze_hHJJFhe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Gridworld Policy_Iteration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
